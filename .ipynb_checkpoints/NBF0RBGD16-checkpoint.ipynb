{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_eligible_customers(active_as_of_dt):\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"customer_status_history_train.csv.gz\")\n",
    "    status = df.filter(df.snapshot_day <= active_as_of_dt).toPandas()\n",
    "    status.snapshot_day = pd.to_datetime(status.snapshot_day, format='%Y-%m-%d')\n",
    "    prior_month = status[status.snapshot_day <= active_as_of_dt].groupby('customer_id')\n",
    "    # pick the last status of the month\n",
    "    x = prior_month.last()\n",
    "    active_customers = x[x.ku_status == 'PAID ACTIVE'].index\n",
    "    return active_customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# prediction_dt is 1 month after active_as_of_dt and they should be month-ends\n",
    "def get_customer_status(prediction_dt, active_as_of_dt):\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_status_history_train.csv.gz\")\n",
    "    status = df.filter(df.snapshot_day <= prediction_dt).toPandas()\n",
    "    status.snapshot_day = pd.to_datetime(status.snapshot_day, format='%Y-%m-%d')\n",
    "    prior_month = status[status.snapshot_day <= active_as_of_dt].groupby('customer_id')\n",
    "    # pick the last status of the month\n",
    "    x = prior_month.last()\n",
    "    active_customers = x[x.ku_status == 'PAID ACTIVE'].index\n",
    "    \n",
    "    prediction_month = status[status.snapshot_day <= prediction_dt].groupby('customer_id')\n",
    "    # set the last status of the month (in case the status flipped within the month)\n",
    "    x = prediction_month.last()\n",
    "    prediction_month_status = x[x.index.isin(active_customers)]\n",
    "    return prediction_month_status['ku_status']\n",
    "\n",
    "\n",
    "# get_customer_status('2017-08-31', '2017-07-31').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Returns the subscription age (number of days) for all active customers as of the date provided\n",
    "def get_customer_subscription_age(as_of_dt):\n",
    "    # TODO Accept a pandas frame to avoid the spark s3 read again\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_status_history_train.csv.gz\")\n",
    "    status = df.filter(df.snapshot_day <= as_of_dt).toPandas()\n",
    "    x = status.groupby('customer_id').last()\n",
    "    x = x[x.ku_status == 'PAID ACTIVE']\n",
    "    x['tgt_dt'] = as_of_dt\n",
    "    x.snapshot_day = pd.to_datetime(x.snapshot_day, format='%Y-%m-%d')\n",
    "    x.tgt_dt = pd.to_datetime(x.tgt_dt, format='%Y-%m-%d')\n",
    "    # Subtract when they became last active from the as_of_dt\n",
    "    x['subs_age'] = (x['tgt_dt'] - x['snapshot_day']).dt.days\n",
    "    \n",
    "    return x['subs_age']\n",
    "\n",
    "\n",
    "# get_customer_subscription_age('2017-07-31').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStatusAttributes(as_of_dt):\n",
    "    \n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_status_history_train.csv.gz\")\n",
    "    statusA = df.filter(df.snapshot_day <= as_of_dt).toPandas()\n",
    "    statusA.snapshot_day = pd.to_datetime(statusA.snapshot_day, format='%Y-%m-%d')\n",
    "    statusvals = statusA.groupby(['customer_id','ku_status']).size()\n",
    "    \n",
    "    prior_month = statusA.groupby('customer_id')\n",
    "    numSnaps = prior_month.size()\n",
    "    xr = prior_month.last()\n",
    "    xr['ku_status'] = xr['ku_status'].map({'PAID ACTIVE': 1, 'PAID CANCELLED': 0})\n",
    "    \n",
    "    numSnaps = prior_month.size()\n",
    "    statusvals = statusA.groupby(['customer_id','ku_status']).size()\n",
    "    \n",
    "    cancelledRatio = []\n",
    "    snapVals = []\n",
    "    statusvals.head(10)\n",
    "    j=0\n",
    "    for i, row in xr.iterrows():\n",
    "        try:\n",
    "            paidCancelled = statusvals[i,'PAID CANCELLED']\n",
    "        except KeyError:\n",
    "            paidCancelled = 0\n",
    "        try:\n",
    "            paidActive = statusvals[i,'PAID ACTIVE']\n",
    "        except KeyError:\n",
    "            paidActive = 0\n",
    "        total = paidActive + paidCancelled\n",
    "        snaps = numSnaps[i]\n",
    "        snapVals.append(snaps)\n",
    "        cancelledRatio.append(float(paidCancelled)/float(total))\n",
    "\n",
    "    se = pd.Series(cancelledRatio)\n",
    "    xr['cancelledRatio'] = se.values\n",
    "    se = pd.Series(snapVals)\n",
    "    xr['snapVals'] = se.values\n",
    "    \n",
    "    xr.drop(['ku_status'], axis=1, inplace=True)\n",
    "    \n",
    "    return xr\n",
    "    \n",
    "# getStatusAttributes('2017-07-31').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_customer_activity_features(month_end_d,suffix = \"\"):\n",
    "    # Read the purchase history file in\n",
    "    ph = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_purchase_history.csv.gz\")\n",
    "    # Cheat by filtering on the month end summary record. Pick only paid units and ku loans month-to-date\n",
    "    cust_features = ph.filter(ph.snapshot_day == month_end_d).select(['customer_id', 'paid_units_mtd', 'ku_loans_t7', 'ku_loans_mtd', 'free_units_mtd', 'pbook_units_mtd']).toPandas()\n",
    "    # Convert to integer type\n",
    "    cust_features.paid_units_mtd = cust_features.paid_units_mtd.astype(int)\n",
    "    cust_features.ku_loans_mtd = cust_features.ku_loans_mtd.astype(int)\n",
    "    cust_features.ku_loans_t7 = cust_features.ku_loans_t7.astype(int)\n",
    "    cust_features.free_units_mtd = cust_features.free_units_mtd.astype(int)\n",
    "    cust_features.pbook_units_mtd = cust_features.pbook_units_mtd.astype(int)\n",
    "    \n",
    "    if suffix!=\"\":\n",
    "        cust_features.rename(columns={'paid_units_mtd': 'paid_units_mtd_'+suffix, 'ku_loans_t7': 'ku_loans_t7_'+suffix, 'ku_loans_mtd': 'ku_loans_mtd_'+suffix, 'free_units_mtd': 'free_units_mtd_'+suffix, 'pbook_units_mtd': 'pbook_units_mtd_'+suffix}, inplace=True)\n",
    "\n",
    "\n",
    "    cust_features = cust_features.set_index('customer_id', drop=True)\n",
    "    return cust_features\n",
    "    # cust_features[(cust_features.paid_units_mtd != 0) & (cust_features.ku_loans_mtd != 0)].count()\n",
    "\n",
    "\n",
    "# get_customer_activity_features('2017-07-31').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGradientFeatures(xs):\n",
    "    xs['diffm5tom4kuloans'] = xs['ku_loans_mtd_monthminus5'] - xs['ku_loans_mtd_monthminus4']\n",
    "    xs['diffm4tom3kuloans'] = xs['ku_loans_mtd_monthminus4'] - xs['ku_loans_mtd_monthminus3']\n",
    "    xs['diffm3tom2kuloans'] = xs['ku_loans_mtd_monthminus3'] - xs['ku_loans_mtd_monthminus2']\n",
    "    xs['diffm2tom1kuloans'] = xs['ku_loans_mtd_monthminus2'] - xs['ku_loans_mtd_monthminus1']\n",
    "    xs['diffm1tom0kuloans'] = xs['ku_loans_mtd_monthminus1'] - xs['ku_loans_mtd']\n",
    "    \n",
    "    xs['diffm5tom4paidunits'] = xs['paid_units_mtd_monthminus5'] - xs['paid_units_mtd_monthminus4']\n",
    "    xs['diffm4tom3paidunits'] = xs['paid_units_mtd_monthminus4'] - xs['paid_units_mtd_monthminus3']\n",
    "    xs['diffm3tom2paidunits'] = xs['paid_units_mtd_monthminus3'] - xs['paid_units_mtd_monthminus2']\n",
    "    xs['diffm2tom1paidunits'] = xs['paid_units_mtd_monthminus2'] - xs['paid_units_mtd_monthminus1']\n",
    "    xs['diffm1tom0paidunits'] = xs['paid_units_mtd_monthminus1'] - xs['paid_units_mtd']\n",
    "    return xs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get the count of books read (some part of) in the trailing n days \n",
    "def get_books_read(as_of_dt, n):\n",
    "    reading_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_reading_history.csv.gz\")\n",
    "    # Filter to include only trailing n days from as_of_dt.\n",
    "    end = datetime.strptime(as_of_dt, \"%Y-%m-%d\")\n",
    "    start = end - timedelta(days=n)\n",
    "    # read_hist = reading_df.filter((reading_df.recv_day >= start.strftime('%Y-%m-%d')) & (reading_df.recv_day <= end.strftime('%Y-%m-%d')) ).toPandas()\n",
    "    read_hist = reading_df.filter((reading_df.recv_day >= start) & (reading_df.recv_day <= end) ).toPandas()\n",
    "\n",
    "    # Extract id, asin, max_read,\n",
    "    # group by id, asin and get the last row to find out how far he reached in all the books he read in the month.\n",
    "    u = read_hist[['customer_id', 'asin', 'max_read']].groupby(['customer_id', 'asin']).last().reset_index()\n",
    "    u.head()\n",
    "    # Print the count of books seen in the last 30 days\n",
    "    v = u.reset_index().groupby(['customer_id'])['asin'].count()\n",
    "    \n",
    "    # Filter only the books completed above 95%\n",
    "    # u.max_read = u.max_read.astype('float')\n",
    "    # u[u.max_read > 0.95].groupby(['customer_id'])['asin'].count().head()\n",
    "    return v\n",
    "\n",
    "# get_books_read('2017-07-31', 30)\n",
    "# get_books_read('2017-07-31', 90)\n",
    "# get_books_read('2017-07-31', 180).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_customer_device_features(month_end_dt):\n",
    "    # Read the device history file in\n",
    "    dh = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_device_history.csv.gz\")\n",
    "\n",
    "    cust_features = dh.filter(dh.snapshot_day <= month_end_dt).select(['snapshot_day', 'customer_id', 'active_android', 'active_eink', 'active_kcp', 'active_ios', 'active_kcr', 'active_tablet', 'active_phone']).toPandas()\n",
    "    # Convert to integer type\n",
    "    cust_features.active_android = cust_features.active_android.astype(float)\n",
    "    cust_features.active_eink = cust_features.active_eink.astype(float)\n",
    "    cust_features.active_kcp = cust_features.active_kcp.astype(float)\n",
    "    cust_features.active_ios = cust_features.active_ios.astype(float)\n",
    "    cust_features.active_kcr = cust_features.active_kcr.astype(float)\n",
    "    cust_features.active_tablet = cust_features.active_tablet.astype(float)\n",
    "    cust_features.active_phone = cust_features.active_phone.astype(float)\n",
    "    cust_features.fillna(0, inplace=True)\n",
    "    cust_features.active_android = cust_features.active_android.astype(int)\n",
    "    cust_features.active_eink = cust_features.active_eink.astype(int)\n",
    "    cust_features.active_kcp = cust_features.active_kcp.astype(int)\n",
    "    cust_features.active_ios = cust_features.active_ios.astype(int)\n",
    "    cust_features.active_kcr = cust_features.active_kcr.astype(int)\n",
    "    cust_features.active_tablet = cust_features.active_tablet.astype(int)\n",
    "    cust_features.active_phone = cust_features.active_phone.astype(int)\n",
    "    cust_features = cust_features.groupby('customer_id')[['active_android', 'active_eink', 'active_kcp', 'active_ios', 'active_kcr', 'active_tablet', 'active_phone']].last()\n",
    "    return cust_features\n",
    "    # cust_features[(cust_features.paid_units_mtd != 0) & (cust_features.ku_loans_mtd != 0)].count()\n",
    "\n",
    "\n",
    "# get_customer_device_features('2017-07-31').head()\n",
    "# get_customer_device_features('2017-06-30').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to save intermediate data frames to CSV\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "from boto3.session import Session\n",
    "\n",
    "# To be used in eider workspace only (references variables provided by eider)\n",
    "def write_to_s3(df, bucket, key):\n",
    "    session = Session(aws_access_key_id=awsAccessKeyId, aws_secret_access_key=awsSecretKey)\n",
    "    s3_resource = session.resource(\"s3\")\n",
    "    csv_buffer = BytesIO()\n",
    "    df.to_csv(csv_buffer)\n",
    "    # s3_resource.Object('emr-eldorado', 'cust_features.csv').put(Body=csv_buffer.getvalue())\n",
    "    s3_resource.Object(bucket, username + \"/\" + key).put(Body=csv_buffer.getvalue())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine all the data into a single dataframe for use in training\n",
    "def create_training_dataset(as_of_dt):\n",
    "\n",
    "    candidates = get_eligible_customers(as_of_dt)\n",
    "    d1 = get_customer_subscription_age(as_of_dt)\n",
    "    d11 = getStatusAttributes(as_of_dt)\n",
    "    d2 = get_books_read(as_of_dt, 30)\n",
    "    d2.name = 'books_30'\n",
    "    d3 = get_books_read(as_of_dt, 90)\n",
    "    d3.name = 'books_90'\n",
    "    # d4 = get_books_read(as_of_dt, 180)\n",
    "    d4 = get_customer_activity_features(as_of_dt)\n",
    "    d5 = get_customer_activity_features('2017-06-30',\"monthminus1\")\n",
    "    \n",
    "    d6 = get_customer_activity_features('2017-05-31', \"monthminus2\")\n",
    "    \n",
    "    d7 = get_customer_activity_features('2017-04-30', \"monthminus3\")\n",
    "    \n",
    "    d8 = get_customer_activity_features('2017-03-31', \"monthminus4\")\n",
    "    \n",
    "    d9 = get_customer_activity_features('2017-02-28', \"monthminus5\")\n",
    "    \n",
    "    d10 = get_customer_device_features('2017-07-31')\n",
    "    \n",
    "    df = d1.to_frame().join(d2.to_frame(), how='left').join(d3.to_frame(), how='left').join(d11, how='left').join(d4, how='left').join(d5, how='left').join(d6, how='left').join(d7, how='left').join(d8, how='left').join(d9, how='left').join(d10, how='left')\n",
    "    df = df.fillna(0)\n",
    "    df = getGradientFeatures(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# df = create_training_dataset('2017-07-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "def create_model(dfx):\n",
    "\n",
    "    # target, features = dmatrices('ku_status ~ subs_age + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd',\n",
    "    #                   dfx, return_type=\"dataframe\")\n",
    "    difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n",
    "    difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n",
    "    target, features = dmatrices('ku_status ~ subs_age + cancelledRatio + snapVals + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd + pbook_units_mtd + diffm5tom4paidunits + diffm4tom3paidunits + diffm3tom2paidunits + diffm2tom1paidunits + diffm1tom0paidunits + diffm5tom4kuloans + diffm4tom3kuloans + diffm3tom2kuloans + diffm2tom1kuloans + diffm1tom0kuloans + active_android + active_eink + active_kcp + active_ios + active_kcr + active_tablet + active_phone',\n",
    "                      dfx, return_type=\"dataframe\")\n",
    "    features = features.drop('Intercept', axis=1)\n",
    "    print features.columns\n",
    "    print target.head()\n",
    "    \n",
    "    # Set 1 to indicte customer churn, 0 otherwise\n",
    "    target['status'] = target['ku_status[PAID CANCELLED]']\n",
    "    target = target['status']\n",
    "    # make it a 1d array\n",
    "    target = np.ravel(target)\n",
    "    print target.size\n",
    "    \n",
    "    #Split into training and test sets\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=0)\n",
    "    \n",
    "    # instantiate a logistic regression model, and fit with features and target\n",
    "    model = LogisticRegression()\n",
    "    model = model.fit(features_train, target_train)\n",
    "    \n",
    "    # predict the outcome for the test features\n",
    "    predicted = model.predict(features_test)\n",
    "    print predicted\n",
    "    \n",
    "    # Check to see if it identifies anyone as churning\n",
    "    predicted.size\n",
    "    np.sum(predicted)\n",
    "    \n",
    "    probs = model.predict_proba(features_test)\n",
    "    print probs\n",
    "    \n",
    "    print metrics.accuracy_score(target_test, predicted)\n",
    "    # The metric we use in leaderboard\n",
    "    print metrics.roc_auc_score(target_test, probs[:, 1])\n",
    "    \n",
    "    print metrics.confusion_matrix(target_test, predicted)\n",
    "    print metrics.classification_report(target_test, predicted)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def createRandomForestmodel(dfx):\n",
    "\n",
    "    # target, features = dmatrices('ku_status ~ subs_age + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd',\n",
    "    #                   dfx, return_type=\"dataframe\")\n",
    "    difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n",
    "    difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n",
    "    target, features = dmatrices('ku_status ~ subs_age + cancelledRatio + snapVals + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd + pbook_units_mtd + diffm5tom4paidunits + diffm4tom3paidunits + diffm3tom2paidunits + diffm2tom1paidunits + diffm1tom0paidunits + diffm5tom4kuloans + diffm4tom3kuloans + diffm3tom2kuloans + diffm2tom1kuloans + diffm1tom0kuloans + active_android + active_eink + active_kcp + active_ios + active_kcr + active_tablet + active_phone',\n",
    "                      dfx, return_type=\"dataframe\")\n",
    "    features = features.drop('Intercept', axis=1)\n",
    "    print features.columns\n",
    "    print target.head()\n",
    "    \n",
    "    # Set 1 to indicte customer churn, 0 otherwise\n",
    "    target['status'] = target['ku_status[PAID CANCELLED]']\n",
    "    target = target['status']\n",
    "    # make it a 1d array\n",
    "    target = np.ravel(target)\n",
    "    print target.size\n",
    "    \n",
    "    #Split into training and test sets\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=0)\n",
    "    \n",
    "    max_depth = [i for i in range(5,20)]\n",
    "    min_samples_split = [i for i in range(4,20)]\n",
    "    n_estimators = [30]\n",
    "    parameters = {'max_depth':max_depth,'min_samples_split':min_samples_split,'n_estimators':n_estimators}\n",
    "    RFC=RandomForestClassifier()\n",
    "    grid_obj = GridSearchCV(RFC, parameters,cv=5)\n",
    "    \n",
    "    # TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "    grid_fit = grid_obj.fit(features_train,target_train)\n",
    "    \n",
    "    # Get the estimator\n",
    "    best_clf = grid_fit.best_estimator_\n",
    "    \n",
    "    print grid_fit.best_params_ \n",
    "    \n",
    "    # Make predictions using the unoptimized and model\n",
    "    model = best_clf.fit(features_train, target_train)\n",
    "    \n",
    "    # predict the outcome for the test features\n",
    "    predicted = model.predict(features_test)\n",
    "    print predicted\n",
    "    \n",
    "    # Check to see if it identifies anyone as churning\n",
    "    predicted.size\n",
    "    np.sum(predicted)\n",
    "    \n",
    "    probs = model.predict_proba(features_test)\n",
    "    print probs\n",
    "    \n",
    "    print metrics.accuracy_score(target_test, predicted)\n",
    "    # The metric we use in leaderboard\n",
    "    print metrics.roc_auc_score(target_test, probs[:, 1])\n",
    "    \n",
    "    print metrics.confusion_matrix(target_test, predicted)\n",
    "    print metrics.classification_report(target_test, predicted)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def createRandomForestmodel(dfx):\n",
    "\n",
    "    # target, features = dmatrices('ku_status ~ subs_age + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd',\n",
    "    #                   dfx, return_type=\"dataframe\")\n",
    "    difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n",
    "    difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n",
    "    target, features = dmatrices('ku_status ~ subs_age + cancelledRatio + snapVals + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd + pbook_units_mtd + diffm5tom4paidunits + diffm4tom3paidunits + diffm3tom2paidunits + diffm2tom1paidunits + diffm1tom0paidunits + diffm5tom4kuloans + diffm4tom3kuloans + diffm3tom2kuloans + diffm2tom1kuloans + diffm1tom0kuloans + active_android + active_eink + active_kcp + active_ios + active_kcr + active_tablet + active_phone',\n",
    "                      dfx, return_type=\"dataframe\")\n",
    "    features = features.drop('Intercept', axis=1)\n",
    "    print features.columns\n",
    "    print target.head()\n",
    "    \n",
    "    # Set 1 to indicte customer churn, 0 otherwise\n",
    "    target['status'] = target['ku_status[PAID CANCELLED]']\n",
    "    target = target['status']\n",
    "    # make it a 1d array\n",
    "    target = np.ravel(target)\n",
    "    print target.size\n",
    "    \n",
    "    #Split into training and test sets\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=0)\n",
    "    \n",
    "    max_depth = [i for i in range(5,20)]\n",
    "    min_samples_split = [i for i in range(4,20)]\n",
    "    n_estimators = [30]\n",
    "    parameters = {'max_depth':max_depth,'min_samples_split':min_samples_split,'n_estimators':n_estimators}\n",
    "    RFC=RandomForestClassifier()\n",
    "    grid_obj = GridSearchCV(RFC, parameters,cv=5)\n",
    "    \n",
    "    # TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "    grid_fit = grid_obj.fit(features_train,target_train)\n",
    "    \n",
    "    # Get the estimator\n",
    "    best_clf = grid_fit.best_estimator_\n",
    "    \n",
    "    print grid_fit.best_params_ \n",
    "    \n",
    "    # Make predictions using the unoptimized and model\n",
    "    model = best_clf.fit(features_train, target_train)\n",
    "    \n",
    "    # predict the outcome for the test features\n",
    "    predicted = model.predict(features_test)\n",
    "    print predicted\n",
    "    \n",
    "    # Check to see if it identifies anyone as churning\n",
    "    predicted.size\n",
    "    np.sum(predicted)\n",
    "    \n",
    "    probs = model.predict_proba(features_test)\n",
    "    print probs\n",
    "    \n",
    "    print metrics.accuracy_score(target_test, predicted)\n",
    "    # The metric we use in leaderboard\n",
    "    print metrics.roc_auc_score(target_test, probs[:, 1])\n",
    "    \n",
    "    print metrics.confusion_matrix(target_test, predicted)\n",
    "    print metrics.classification_report(target_test, predicted)\n",
    "    return model\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def createGradientBoostingModel(dfx):\n",
    "\n",
    "    # target, features = dmatrices('ku_status ~ subs_age + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd',\n",
    "    #                   dfx, return_type=\"dataframe\")\n",
    "    difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n",
    "    difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n",
    "    target, features = dmatrices('ku_status ~ subs_age + cancelledRatio + snapVals + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd + pbook_units_mtd + diffm5tom4paidunits + diffm4tom3paidunits + diffm3tom2paidunits + diffm2tom1paidunits + diffm1tom0paidunits + diffm5tom4kuloans + diffm4tom3kuloans + diffm3tom2kuloans + diffm2tom1kuloans + diffm1tom0kuloans + active_android + active_eink + active_kcp + active_ios + active_kcr + active_tablet + active_phone',\n",
    "                      dfx, return_type=\"dataframe\")\n",
    "    features = features.drop('Intercept', axis=1)\n",
    "    print features.columns\n",
    "    print target.head()\n",
    "    \n",
    "    # Set 1 to indicte customer churn, 0 otherwise\n",
    "    target['status'] = target['ku_status[PAID CANCELLED]']\n",
    "    target = target['status']\n",
    "    # make it a 1d array\n",
    "    target = np.ravel(target)\n",
    "    print target.size\n",
    "    \n",
    "    #Split into training and test sets\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=0)\n",
    "    \n",
    "    max_depth = [i for i in range(5,20)]\n",
    "    min_samples_split = [i for i in range(4,20)]\n",
    "    n_estimators = [30]\n",
    "    parameters = {'max_depth':max_depth,'min_samples_split':min_samples_split,'n_estimators':n_estimators}\n",
    "    GBC=GradientBoostingClassifier()\n",
    "    grid_obj = GridSearchCV(GBC, parameters,cv=5)\n",
    "    \n",
    "    # TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "    grid_fit = grid_obj.fit(features_train,target_train)\n",
    "    \n",
    "    # Get the estimator\n",
    "    best_clf_gb = grid_fit.best_estimator_\n",
    "    \n",
    "    print grid_fit.best_params_ \n",
    "    \n",
    "    # Make predictions using the unoptimized and model\n",
    "    model = best_clf_gb.fit(features_train, target_train)\n",
    "    \n",
    "    # predict the outcome for the test features\n",
    "    predicted = model.predict(features_test)\n",
    "    print predicted\n",
    "    \n",
    "    # Check to see if it identifies anyone as churning\n",
    "    predicted.size\n",
    "    np.sum(predicted)\n",
    "    \n",
    "    probs = model.predict_proba(features_test)\n",
    "    print probs\n",
    "    \n",
    "    print metrics.accuracy_score(target_test, predicted)\n",
    "    # The metric we use in leaderboard\n",
    "    print metrics.roc_auc_score(target_test, probs[:, 1])\n",
    "    \n",
    "    print metrics.confusion_matrix(target_test, predicted)\n",
    "    print metrics.classification_report(target_test, predicted)\n",
    "    return model\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model using data till 2017-07-31\n",
    "# Create the training dataset\n",
    "df = create_training_dataset('2017-07-31')\n",
    "actual_status = get_customer_status('2017-08-31', '2017-07-31')\n",
    "# Join with the actual status\n",
    "dfx = df.join(actual_status.to_frame(), how='left')\n",
    "dfx.head()\n",
    "# Save it in s3\n",
    "write_to_s3(dfx, 'ml-hack-subs-data', 'training-dataset-2017-07-31.csv')\n",
    "# To retrieve the file via Eider save it in /tmp\n",
    "dfx.to_csv('/tmp/training-dataset-2017-07-31.csv')\n",
    "\n",
    "# Train the model and get the artifact\n",
    "# model = create_model(dfx)\n",
    "model = createRandomForestmodel(dfx)\n",
    "# model = createGradientBoostingModel(dfx)\n",
    "\n",
    "# Create the feature data for the validation set\n",
    "df_contest = create_training_dataset('2017-08-31')\n",
    "\n",
    "# Filter the feature df to include only the features used in the model\n",
    "difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n",
    "difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n",
    "deviceFeatures = ['active_android', 'active_eink', 'active_kcp', 'active_ios', 'active_kcr', 'active_tablet', 'active_phone']\n",
    "contest_input = df_contest[['subs_age', 'cancelledRatio', 'snapVals', 'books_30', 'books_90', 'ku_loans_t7', 'ku_loans_mtd', 'pbook_units_mtd'] + difffeatures + deviceFeatures + difffeaturespaidunits]\n",
    "# Predict the probabilities\n",
    "contest_probs = model.predict_proba(contest_input)\n",
    "# Create the submission file in the required format\n",
    "submission = pd.DataFrame(contest_probs[:,1])\n",
    "submission.columns = ['ku_status']\n",
    "submission.set_index(contest_input.index, inplace=True)\n",
    "submission.index.name = 'ID'\n",
    "submission.head()\n",
    "# Save the file to Eider TMP, to retrieve it from the FILES page\n",
    "submission.to_csv('/tmp/contest-submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submission.head(5)\n",
    "submission.to_csv('/tmp/contest-submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
