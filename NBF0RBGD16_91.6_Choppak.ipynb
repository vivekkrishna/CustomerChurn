{
  "metadata" : {
    "kernelspec" : {
      "display_name" : "Python 2",
      "language" : "python",
      "name" : "python2"
    },
    "language_info" : {
      "file_extension" : ".py",
      "mimetype" : "text/x-python",
      "name" : "python"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 2,
  "cells" : [ {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import pandas as pd\ndef get_eligible_customers(active_as_of_dt):\n    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_status_history_train.csv.gz\")\n    status = df.filter(df.snapshot_day <= active_as_of_dt).toPandas()\n    status.snapshot_day = pd.to_datetime(status.snapshot_day, format='%Y-%m-%d')\n    prior_month = status[status.snapshot_day <= active_as_of_dt].groupby('customer_id')\n    # pick the last status of the month\n    x = prior_month.last()\n    active_customers = x[x.ku_status == 'PAID ACTIVE'].index\n    return active_customers\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import pandas as pd\npd.set_option('display.max_colwidth', -1)\n\n# prediction_dt is 1 month after active_as_of_dt and they should be month-ends\ndef get_customer_status(prediction_dt, active_as_of_dt):\n    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_status_history_train.csv.gz\")\n    status = df.filter(df.snapshot_day <= prediction_dt).toPandas()\n    status.snapshot_day = pd.to_datetime(status.snapshot_day, format='%Y-%m-%d')\n    prior_month = status[status.snapshot_day <= active_as_of_dt].groupby('customer_id')\n    # pick the last status of the month\n    x = prior_month.last()\n    active_customers = x[x.ku_status == 'PAID ACTIVE'].index\n    \n    prediction_month = status[status.snapshot_day <= prediction_dt].groupby('customer_id')\n    # set the last status of the month (in case the status flipped within the month)\n    x = prediction_month.last()\n    prediction_month_status = x[x.index.isin(active_customers)]\n    return prediction_month_status['ku_status']\n\n\n# get_customer_status('2017-08-31', '2017-07-31').head()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import pandas as pd\n# Returns the subscription age (number of days) for all active customers as of the date provided\ndef get_customer_subscription_age(as_of_dt):\n    # TODO Accept a pandas frame to avoid the spark s3 read again\n    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_status_history_train.csv.gz\")\n    status = df.filter(df.snapshot_day <= as_of_dt).toPandas()\n    x = status.groupby('customer_id').last()\n    x = x[x.ku_status == 'PAID ACTIVE']\n    x['tgt_dt'] = as_of_dt\n    x.snapshot_day = pd.to_datetime(x.snapshot_day, format='%Y-%m-%d')\n    x.tgt_dt = pd.to_datetime(x.tgt_dt, format='%Y-%m-%d')\n    # Subtract when they became last active from the as_of_dt\n    x['subs_age'] = (x['tgt_dt'] - x['snapshot_day']).dt.days\n    \n    return x['subs_age']\n\n\n# get_customer_subscription_age('2017-07-31').head()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def getStatusAttributes(as_of_dt):\n    \n    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_status_history_train.csv.gz\")\n    statusA = df.filter(df.snapshot_day <= as_of_dt).toPandas()\n    statusA.snapshot_day = pd.to_datetime(statusA.snapshot_day, format='%Y-%m-%d')\n    statusvals = statusA.groupby(['customer_id','ku_status']).size()\n    \n    prior_month = statusA.groupby('customer_id')\n    numSnaps = prior_month.size()\n    xr = prior_month.last()\n    xr['ku_status'] = xr['ku_status'].map({'PAID ACTIVE': 1, 'PAID CANCELLED': 0})\n    \n    numSnaps = prior_month.size()\n    statusvals = statusA.groupby(['customer_id','ku_status']).size()\n    \n    cancelledRatio = []\n    snapVals = []\n    statusvals.head(10)\n    j=0\n    for i, row in xr.iterrows():\n        try:\n            paidCancelled = statusvals[i,'PAID CANCELLED']\n        except KeyError:\n            paidCancelled = 0\n        try:\n            paidActive = statusvals[i,'PAID ACTIVE']\n        except KeyError:\n            paidActive = 0\n        total = paidActive + paidCancelled\n        snaps = numSnaps[i]\n        snapVals.append(snaps)\n        cancelledRatio.append(float(paidCancelled)/float(total))\n\n    se = pd.Series(cancelledRatio)\n    xr['cancelledRatio'] = se.values\n    se = pd.Series(snapVals)\n    xr['snapVals'] = se.values\n    \n    xr.drop(['ku_status'], axis=1, inplace=True)\n    \n    res = prior_month['snapshot_day'].agg({'enter': 'first', 'exit': 'last'})\n\n    res['time_diff'] = res['exit'] - res['enter']\n\n    res.time_diff = res.time_diff.dt.days\n    \n    res.drop(['enter','exit'], axis=1, inplace=True)\n\n    xrl = xr.join(res, how='left')\n\n    return xrl\n    \n# getStatusAttributes('2017-07-31').head(5)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import pandas as pd\ndef get_customer_activity_features(month_end_d,suffix = \"\"):\n    # Read the purchase history file in\n    ph = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_purchase_history.csv.gz\")\n    # Cheat by filtering on the month end summary record. Pick only paid units and ku loans month-to-date\n    cust_features = ph.filter(ph.snapshot_day == month_end_d).select(['customer_id', 'paid_units_mtd', 'ku_loans_t7', 'ku_loans_mtd', 'free_units_mtd', 'pbook_units_mtd']).toPandas()\n    # Convert to integer type\n    cust_features.paid_units_mtd = cust_features.paid_units_mtd.astype(int)\n    cust_features.ku_loans_mtd = cust_features.ku_loans_mtd.astype(int)\n    cust_features.ku_loans_t7 = cust_features.ku_loans_t7.astype(int)\n    cust_features.free_units_mtd = cust_features.free_units_mtd.astype(int)\n    cust_features.pbook_units_mtd = cust_features.pbook_units_mtd.astype(int)\n    \n    if suffix!=\"\":\n        cust_features.rename(columns={'paid_units_mtd': 'paid_units_mtd_'+suffix, 'ku_loans_t7': 'ku_loans_t7_'+suffix, 'ku_loans_mtd': 'ku_loans_mtd_'+suffix, 'free_units_mtd': 'free_units_mtd_'+suffix, 'pbook_units_mtd': 'pbook_units_mtd_'+suffix}, inplace=True)\n\n\n    cust_features = cust_features.set_index('customer_id', drop=True)\n    return cust_features\n    # cust_features[(cust_features.paid_units_mtd != 0) & (cust_features.ku_loans_mtd != 0)].count()\n\n\n# get_customer_activity_features('2017-07-31').head()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def getGradientFeatures(xs):\n    xs['diffm7tom6kuloans'] = xs['ku_loans_mtd_monthminus7'] - xs['ku_loans_mtd_monthminus6']\n    xs['diffm6tom5kuloans'] = xs['ku_loans_mtd_monthminus6'] - xs['ku_loans_mtd_monthminus5']\n    xs['diffm5tom4kuloans'] = xs['ku_loans_mtd_monthminus5'] - xs['ku_loans_mtd_monthminus4']\n    xs['diffm4tom3kuloans'] = xs['ku_loans_mtd_monthminus4'] - xs['ku_loans_mtd_monthminus3']\n    xs['diffm3tom2kuloans'] = xs['ku_loans_mtd_monthminus3'] - xs['ku_loans_mtd_monthminus2']\n    xs['diffm2tom1kuloans'] = xs['ku_loans_mtd_monthminus2'] - xs['ku_loans_mtd_monthminus1']\n    xs['diffm1tom0kuloans'] = xs['ku_loans_mtd_monthminus1'] - xs['ku_loans_mtd']\n    \n    xs['diffm5tom4paidunits'] = xs['paid_units_mtd_monthminus5'] - xs['paid_units_mtd_monthminus4']\n    xs['diffm4tom3paidunits'] = xs['paid_units_mtd_monthminus4'] - xs['paid_units_mtd_monthminus3']\n    xs['diffm3tom2paidunits'] = xs['paid_units_mtd_monthminus3'] - xs['paid_units_mtd_monthminus2']\n    xs['diffm2tom1paidunits'] = xs['paid_units_mtd_monthminus2'] - xs['paid_units_mtd_monthminus1']\n    xs['diffm1tom0paidunits'] = xs['paid_units_mtd_monthminus1'] - xs['paid_units_mtd']\n    \n    xs['diffm5tom4pbookunits'] = xs['pbook_units_mtd_monthminus5'] - xs['pbook_units_mtd_monthminus4']\n    xs['diffm4tom3pbookunits'] = xs['pbook_units_mtd_monthminus4'] - xs['pbook_units_mtd_monthminus3']\n    xs['diffm3tom2pbookunits'] = xs['pbook_units_mtd_monthminus3'] - xs['pbook_units_mtd_monthminus2']\n    xs['diffm2tom1pbookunits'] = xs['pbook_units_mtd_monthminus2'] - xs['pbook_units_mtd_monthminus1']\n    xs['diffm1tom0pbookunits'] = xs['pbook_units_mtd_monthminus1'] - xs['pbook_units_mtd']\n    \n    xs['diffm5tom4fbookunits'] = xs['free_units_mtd_monthminus5'] - xs['free_units_mtd_monthminus4']\n    xs['diffm4tom3fbookunits'] = xs['free_units_mtd_monthminus4'] - xs['free_units_mtd_monthminus3']\n    xs['diffm3tom2fbookunits'] = xs['free_units_mtd_monthminus3'] - xs['free_units_mtd_monthminus2']\n    xs['diffm2tom1fbookunits'] = xs['free_units_mtd_monthminus2'] - xs['free_units_mtd_monthminus1']\n    xs['diffm1tom0fbookunits'] = xs['free_units_mtd_monthminus1'] - xs['free_units_mtd']\n    \n    return xs\n    " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from datetime import datetime, timedelta\r\n\r\n# Get the count of books read (some part of) in the trailing n days \r\ndef get_books_read(as_of_dt, n):\r\n    reading_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_reading_history.csv.gz\")\r\n    # Filter to include only trailing n days from as_of_dt.\r\n    end = datetime.strptime(as_of_dt, \"%Y-%m-%d\")\r\n    start = end - timedelta(days=n)\r\n    # read_hist = reading_df.filter((reading_df.recv_day >= start.strftime('%Y-%m-%d')) & (reading_df.recv_day <= end.strftime('%Y-%m-%d')) ).toPandas()\r\n    read_hist = reading_df.filter((reading_df.recv_day >= start) & (reading_df.recv_day <= end) ).toPandas()\r\n\r\n    # Extract id, asin, max_read,\r\n    # group by id, asin and get the last row to find out how far he reached in all the books he read in the month.\r\n    u = read_hist[['customer_id', 'asin', 'max_read']].groupby(['customer_id', 'asin']).last().reset_index()\r\n    u.head()\r\n    # Print the count of books seen in the last 30 days\r\n    v = u.reset_index().groupby(['customer_id'])['asin'].count()\r\n\r\n    return v\r\n\r\n# get_books_read('2017-07-31', 30)\r\n# get_books_read('2017-07-31', 90)\r\n# get_books_read('2017-07-31', 180).head()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def get_books_maxread(as_of_dt, n):\r\n    reading_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_reading_history.csv.gz\")\r\n    # Filter to include only trailing n days from as_of_dt.\r\n    end = datetime.strptime(as_of_dt, \"%Y-%m-%d\")\r\n    start = end - timedelta(days=n)\r\n    # read_hist = reading_df.filter((reading_df.recv_day >= start.strftime('%Y-%m-%d')) & (reading_df.recv_day <= end.strftime('%Y-%m-%d')) ).toPandas()\r\n    read_hist = reading_df.filter((reading_df.recv_day >= start) & (reading_df.recv_day <= end) ).toPandas()\r\n    \r\n    u = read_hist[['customer_id', 'asin', 'max_read']].groupby(['customer_id', 'asin']).last()#.mean()#.last().reset_index()\r\n    # u.head(50)\r\n    u['max_read'] = u['max_read'].astype(float)\r\n    u2 = u.groupby(level=0).mean()\r\n    # u2 = u.groupby('customer_id').agg({'max_read' : 'mean'})\r\n    # u2.head(50)\r\n    u3 = read_hist[['customer_id', 'asin']]\r\n    u4 = u3.groupby('customer_id')['asin'].apply(list)\r\n    u4.head(50)\r\n    \r\n    # df9 = pd.read_csv(\"s3n://ml-hack-subs-data/program_availability.csv\",sep='\\t')\r\n    # # (df['date'] > '2000-6-1') & (df['date'] <= '2000-6-10')\r\n    # df8 = df9[(df9['end_date']<=end.strftime('%Y-%m-%d')) & (df9['end_date']>=start.strftime('%Y-%m-%d'))]\r\n    \r\n    df9 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/program_availability.csv.gz\")\r\n\r\n    df8 = df9.filter((df9.end_date >= start.strftime('%Y-%m-%d')) & (df9.end_date <= end.strftime('%Y-%m-%d')) ).toPandas()\r\n    \r\n    # df7.head(-30)\r\n    lostAsinsSet = set(df8[\"asin\"].tolist())\r\n\r\n    lostbratio = []\r\n    for (i,row) in u4.iteritems():\r\n        intersect = lostAsinsSet.intersection(row)\r\n        lostbooksratio = float(len(intersect))/float(len(row))\r\n        lostbratio.append(lostbooksratio)\r\n\r\n    se = pd.Series(lostbratio)\r\n    u2['lostbratio'] = se.values\r\n    u2.head()\r\n\r\n    u9 = read_hist[['customer_id', 'start_read', 'end_read']].groupby(['customer_id'])['start_read'].count().reset_index(name=\"bookreads\")#.mean()#.last().reset_index()\r\n    u9.set_index('customer_id', inplace=True)\r\n    u.head(50)\r\n    u2 = u2.join(u9,how='left')\r\n    \r\n    ###############\r\n    \r\n    # read_hist = reading_df.filter(reading_df.recv_day <= end.strftime('%Y-%m-%d')).toPandas())\r\n    read_hist = reading_df.filter(reading_df.recv_day <= end).toPandas()\r\n    \r\n    u99 = read_hist[['customer_id', 'recv_day']].groupby(['customer_id']).last()\r\n\r\n    u99['tgt_dt'] = as_of_dt\r\n    u99.recv_day = pd.to_datetime(u99.recv_day, format='%Y-%m-%d')\r\n    u99.tgt_dt = pd.to_datetime(u99.tgt_dt, format='%Y-%m-%d')\r\n    # Subtract when they became last active from the as_of_dt\r\n    u99['lastReadAge'] = (u99['tgt_dt'] - u99['recv_day']).dt.days\r\n    \r\n    u99.drop(['tgt_dt','recv_day'], axis=1, inplace=True)\r\n    \r\n    u2 = u2.join(u99,how='left')\r\n    \r\n    return u2\r\n    \r\n# get_books_maxread('2017-07-31', 30).head(10)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def get_customer_device_features(month_end_dt):\n    # Read the device history file in\n    dh = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"s3n://ml-hack-subs-data/customer_device_history.csv.gz\")\n\n    cust_features = dh.filter(dh.snapshot_day <= month_end_dt).select(['snapshot_day', 'customer_id', 'active_android', 'active_eink', 'active_kcp', 'active_ios', 'active_kcr', 'active_tablet', 'active_phone']).toPandas()\n    # Convert to integer type\n    cust_features.active_android = cust_features.active_android.astype(float)\n    cust_features.active_eink = cust_features.active_eink.astype(float)\n    cust_features.active_kcp = cust_features.active_kcp.astype(float)\n    cust_features.active_ios = cust_features.active_ios.astype(float)\n    cust_features.active_kcr = cust_features.active_kcr.astype(float)\n    cust_features.active_tablet = cust_features.active_tablet.astype(float)\n    cust_features.active_phone = cust_features.active_phone.astype(float)\n    cust_features.fillna(0, inplace=True)\n    cust_features.active_android = cust_features.active_android.astype(int)\n    cust_features.active_eink = cust_features.active_eink.astype(int)\n    cust_features.active_kcp = cust_features.active_kcp.astype(int)\n    cust_features.active_ios = cust_features.active_ios.astype(int)\n    cust_features.active_kcr = cust_features.active_kcr.astype(int)\n    cust_features.active_tablet = cust_features.active_tablet.astype(int)\n    cust_features.active_phone = cust_features.active_phone.astype(int)\n    cust_features = cust_features.groupby('customer_id')[['active_android', 'active_eink', 'active_kcp', 'active_ios', 'active_kcr', 'active_tablet', 'active_phone']].last()\n    return cust_features\n    # cust_features[(cust_features.paid_units_mtd != 0) & (cust_features.ku_loans_mtd != 0)].count()\n\n\n# get_customer_device_features('2017-07-31').head()\n# get_customer_device_features('2017-06-30').head()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Code to save intermediate data frames to CSV\nfrom io import BytesIO\nimport boto3\nfrom boto3.session import Session\n\n# To be used in eider workspace only (references variables provided by eider)\ndef write_to_s3(df, bucket, key):\n    session = Session(aws_access_key_id=awsAccessKeyId, aws_secret_access_key=awsSecretKey)\n    s3_resource = session.resource(\"s3\")\n    csv_buffer = BytesIO()\n    df.to_csv(csv_buffer)\n    # s3_resource.Object('emr-eldorado', 'cust_features.csv').put(Body=csv_buffer.getvalue())\n    s3_resource.Object(bucket, username + \"/\" + key).put(Body=csv_buffer.getvalue())\n\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def removeOutliers(features):\r\n    \r\n    features_after_removing_outliers = features.copy()\r\n    for feature in features.keys():\r\n\r\n        if feature == 'ku_status':\r\n            continue\r\n        # TODO: Calculate Q1 (25th percentile of the data) for the given feature\r\n        Q1 = np.percentile(features[feature],25)\r\n\r\n        # TODO: Calculate Q3 (75th percentile of the data) for the given feature\r\n        Q3 = np.percentile(features[feature],75)\r\n\r\n        # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\r\n        step = (Q3-Q1)*1.5\r\n\r\n        # Display the outliers\r\n        #print \"Data points considered outliers for the feature '{}':\".format(feature)\r\n        features_after_removing_outliers.drop(features[~((features[feature] >= Q1 - step) & (features[feature] <= Q3 + step))].index)\r\n    return features_after_removing_outliers\r\n# OPTIONAL: Select the indices for data points you wish to remove\r\n# outliers  = [75,154,66]\r\n\r\n# good_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Combine all the data into a single dataframe for use in training\ndef create_training_dataset(as_of_dt):\n\n    candidates = get_eligible_customers(as_of_dt)\n    d1 = get_customer_subscription_age(as_of_dt)\n    d11 = getStatusAttributes(as_of_dt)\n    d2 = get_books_read(as_of_dt, 30)\n    d2.name = 'books_30'\n    d3 = get_books_read(as_of_dt, 90)\n    d3.name = 'books_90'\n    d34 = get_books_read(as_of_dt, 180)\n    d34.name = 'books_180'\n    \n    d31 = get_books_maxread(as_of_dt, 30)\n    \n    d4 = get_customer_activity_features(as_of_dt)\n    d5 = get_customer_activity_features('2017-06-30',\"monthminus1\")\n    \n    d6 = get_customer_activity_features('2017-05-31', \"monthminus2\")\n    \n    d7 = get_customer_activity_features('2017-04-30', \"monthminus3\")\n    \n    d8 = get_customer_activity_features('2017-03-31', \"monthminus4\")\n    \n    d9 = get_customer_activity_features('2017-02-28', \"monthminus5\")\n    \n    d90 = get_customer_activity_features('2017-01-31', \"monthminus6\")\n    \n    d91 = get_customer_activity_features('2016-12-31', \"monthminus7\")\n    \n    d10 = get_customer_device_features('2017-07-31')\n    \n    df = d1.to_frame().join(d2.to_frame(), how='left').join(d3.to_frame(), how='left').join(d34.to_frame(), how='left').join(d31, how='left').join(d11, how='left').join(d4, how='left').join(d5, how='left').join(d6, how='left').join(d7, how='left').join(d8, how='left').join(d9, how='left').join(d90, how='left').join(d91, how='left').join(d10, how='left')\n    df = df.fillna(0)\n    df = getGradientFeatures(df)\n\n    return df\n\n# df = create_training_dataset('2017-07-31')\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom patsy import dmatrices\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nfrom sklearn.cross_validation import cross_val_score\n\ndef create_model(dfx):\n\n    # target, features = dmatrices('ku_status ~ subs_age + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd',\n    #                   dfx, return_type=\"dataframe\")\n    difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n    difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n    difffeaturespbookunits = ['diffm5tom4pbookunits','diffm4tom3pbookunits','diffm3tom2pbookunits','diffm2tom1pbookunits','diffm1tom0pbookunits']\n    difffeaturesfbookunits = ['diffm5tom4fbookunits', 'diffm4tom3fbookunits', 'diffm3tom2fbookunits', 'diffm2tom1fbookunits', 'diffm1tom0fbookunits']\n    target, features = dmatrices('ku_status ~ books_180 + lostbratio + time_diff + max_read + subs_age + cancelledRatio + snapVals + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd + pbook_units_mtd + diffm5tom4paidunits + diffm4tom3paidunits + diffm3tom2paidunits + diffm2tom1paidunits + diffm1tom0paidunits + diffm5tom4kuloans + diffm4tom3kuloans + diffm3tom2kuloans + diffm2tom1kuloans + diffm1tom0kuloans + active_android + active_eink + active_kcp + active_ios + active_kcr + active_tablet + active_phone + diffm5tom4pbookunits + diffm4tom3pbookunits + diffm3tom2pbookunits + diffm2tom1pbookunits + diffm1tom0pbookunits + diffm5tom4fbookunits + diffm4tom3fbookunits + diffm3tom2fbookunits + diffm2tom1fbookunits + diffm1tom0fbookunits',\n                      dfx, return_type=\"dataframe\")\n    features = features.drop('Intercept', axis=1)\n    print features.columns\n    print target.head()\n    \n    # Set 1 to indicte customer churn, 0 otherwise\n    target['status'] = target['ku_status[PAID CANCELLED]']\n    target = target['status']\n    # make it a 1d array\n    target = np.ravel(target)\n    print target.size\n    \n    #Split into training and test sets\n    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=0)\n    \n    # instantiate a logistic regression model, and fit with features and target\n    model = LogisticRegression()\n    model = model.fit(features_train, target_train)\n    \n    # predict the outcome for the test features\n    predicted = model.predict(features_test)\n    print predicted\n    \n    # Check to see if it identifies anyone as churning\n    predicted.size\n    np.sum(predicted)\n    \n    probs = model.predict_proba(features_test)\n    print probs\n    \n    print metrics.accuracy_score(target_test, predicted)\n    # The metric we use in leaderboard\n    print metrics.roc_auc_score(target_test, probs[:, 1])\n    \n    print metrics.confusion_matrix(target_test, predicted)\n    print metrics.classification_report(target_test, predicted)\n    return model\n\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom patsy import dmatrices\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.grid_search import GridSearchCV\n\nfrom sklearn.metrics import fbeta_score, make_scorer\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef createRandomForestmodel(dfx):\n\n    # target, features = dmatrices('ku_status ~ subs_age + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd',\n    #                   dfx, return_type=\"dataframe\")\n    difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n    difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n    difffeaturespbookunits = ['diffm5tom4pbookunits','diffm4tom3pbookunits','diffm3tom2pbookunits','diffm2tom1pbookunits','diffm1tom0pbookunits']\n    target, features = dmatrices('ku_status ~ time_diff + max_read + subs_age + cancelledRatio + snapVals + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd + pbook_units_mtd + diffm5tom4paidunits + diffm4tom3paidunits + diffm3tom2paidunits + diffm2tom1paidunits + diffm1tom0paidunits + diffm5tom4kuloans + diffm4tom3kuloans + diffm3tom2kuloans + diffm2tom1kuloans + diffm1tom0kuloans + active_android + active_eink + active_kcp + active_ios + active_kcr + active_tablet + active_phone + diffm5tom4pbookunits + diffm4tom3pbookunits + diffm3tom2pbookunits + diffm2tom1pbookunits + diffm1tom0pbookunits + diffm5tom4fbookunits + diffm4tom3fbookunits + diffm3tom2fbookunits + diffm2tom1fbookunits + diffm1tom0fbookunits',\n                      dfx, return_type=\"dataframe\")\n    features = features.drop('Intercept', axis=1)\n    print features.columns\n    print target.head()\n    \n    # Set 1 to indicte customer churn, 0 otherwise\n    target['status'] = target['ku_status[PAID CANCELLED]']\n    target = target['status']\n    # make it a 1d array\n    target = np.ravel(target)\n    print target.size\n    \n    #Split into training and test sets\n    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=0)\n    \n    max_depth = [i for i in range(14,24)]\n    min_samples_split = [i for i in range(4,15)]\n    n_estimators = [120]\n    parameters = {'max_depth':max_depth,'min_samples_split':min_samples_split,'n_estimators':n_estimators}\n    RFC=RandomForestClassifier()\n    grid_obj = GridSearchCV(RFC, parameters,cv=5)\n    \n    # TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n    grid_fit = grid_obj.fit(features_train,target_train)\n    \n    # Get the estimator\n    best_clf = grid_fit.best_estimator_\n    \n    print grid_fit.best_params_ \n    \n    # Make predictions using the unoptimized and model\n    model = best_clf.fit(features_train, target_train)\n    \n    # predict the outcome for the test features\n    predicted = model.predict(features_test)\n    print predicted\n    \n    # Check to see if it identifies anyone as churning\n    predicted.size\n    np.sum(predicted)\n    \n    probs = model.predict_proba(features_test)\n    print probs\n    \n    print metrics.accuracy_score(target_test, predicted)\n    # The metric we use in leaderboard\n    print metrics.roc_auc_score(target_test, probs[:, 1])\n    \n    print metrics.confusion_matrix(target_test, predicted)\n    print metrics.classification_report(target_test, predicted)\n    return model\n\n " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom patsy import dmatrices\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.grid_search import GridSearchCV\n\nfrom sklearn.metrics import fbeta_score, make_scorer\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ndef createGradientBoostingModel(dfx):\n\n    # target, features = dmatrices('ku_status ~ subs_age + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd',\n    #                   dfx, return_type=\"dataframe\")\n    difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n    difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n    difffeaturespbookunits = ['diffm5tom4pbookunits','diffm4tom3pbookunits','diffm3tom2pbookunits','diffm2tom1pbookunits','diffm1tom0pbookunits']\n    target, features = dmatrices('ku_status ~ time_diff + max_read + subs_age + cancelledRatio + snapVals + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd + pbook_units_mtd + diffm5tom4paidunits + diffm4tom3paidunits + diffm3tom2paidunits + diffm2tom1paidunits + diffm1tom0paidunits + diffm5tom4kuloans + diffm4tom3kuloans + diffm3tom2kuloans + diffm2tom1kuloans + diffm1tom0kuloans + active_android + active_eink + active_kcp + active_ios + active_kcr + active_tablet + active_phone + diffm5tom4pbookunits + diffm4tom3pbookunits + diffm3tom2pbookunits + diffm2tom1pbookunits + diffm1tom0pbookunits + diffm5tom4fbookunits + diffm4tom3fbookunits + diffm3tom2fbookunits + diffm2tom1fbookunits + diffm1tom0fbookunits',\n                      dfx, return_type=\"dataframe\")\n    features = features.drop('Intercept', axis=1)\n    print features.columns\n    print target.head()\n    \n    # Set 1 to indicte customer churn, 0 otherwise\n    target['status'] = target['ku_status[PAID CANCELLED]']\n    target = target['status']\n    # make it a 1d array\n    target = np.ravel(target)\n    print target.size\n    \n    #Split into training and test sets\n    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=0)\n    \n    max_depth = [i for i in range(14,19)]\n    min_samples_split = [i for i in range(14,19)]\n    n_estimators = [120]\n    learning_rate  = [0.1,0.05]\n    parameters = {'max_depth':max_depth,'min_samples_split':min_samples_split,'n_estimators':n_estimators}\n    GBC=GradientBoostingClassifier()\n    grid_obj = GridSearchCV(GBC, parameters,cv=5)\n    \n    # TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n    grid_fit = grid_obj.fit(features_train,target_train)\n    \n    # Get the estimator\n    best_clf = grid_fit.best_estimator_\n    \n    print grid_fit.best_params_ \n    \n    # Make predictions using the unoptimized and model\n    model = best_clf.fit(features_train, target_train)\n    \n    # predict the outcome for the test features\n    predicted = model.predict(features_test)\n    print predicted\n    \n    # Check to see if it identifies anyone as churning\n    predicted.size\n    np.sum(predicted)\n    \n    probs = model.predict_proba(features_test)\n    print probs\n    \n    print metrics.accuracy_score(target_test, predicted)\n    # The metric we use in leaderboard\n    print metrics.roc_auc_score(target_test, probs[:, 1])\n    \n    print metrics.confusion_matrix(target_test, predicted)\n    print metrics.classification_report(target_test, predicted)\n    return model\n\n " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom patsy import dmatrices\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.grid_search import GridSearchCV\n\nfrom sklearn.metrics import fbeta_score, make_scorer\n\nfrom xgboost.sklearn import XGBClassifier\n\ndef createXGBoostModel(dfx):\n\n    # target, features = dmatrices('ku_status ~ subs_age + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd',\n    #                   dfx, return_type=\"dataframe\")\n    difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n    difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n    difffeaturespbookunits = ['diffm5tom4pbookunits','diffm4tom3pbookunits','diffm3tom2pbookunits','diffm2tom1pbookunits','diffm1tom0pbookunits']\n    target, features = dmatrices('ku_status ~ lastReadAge + bookreads + books_180 + lostbratio + time_diff + max_read + subs_age + cancelledRatio + snapVals + books_30 + books_90 + ku_loans_t7 + ku_loans_mtd + pbook_units_mtd + diffm5tom4paidunits + diffm4tom3paidunits + diffm3tom2paidunits + diffm2tom1paidunits + diffm1tom0paidunits + diffm7tom6kuloans + diffm6tom5kuloans + diffm5tom4kuloans + diffm4tom3kuloans + diffm3tom2kuloans + diffm2tom1kuloans + diffm1tom0kuloans + active_android + active_eink + active_kcp + active_ios + active_kcr + active_tablet + active_phone + diffm5tom4pbookunits + diffm4tom3pbookunits + diffm3tom2pbookunits + diffm2tom1pbookunits + diffm1tom0pbookunits + diffm5tom4fbookunits + diffm4tom3fbookunits + diffm3tom2fbookunits + diffm2tom1fbookunits + diffm1tom0fbookunits',\n                      dfx, return_type=\"dataframe\")\n    features = features.drop('Intercept', axis=1)\n    print features.columns\n    print target.head()\n    \n    # Set 1 to indicte customer churn, 0 otherwise\n    target['status'] = target['ku_status[PAID CANCELLED]']\n    target = target['status']\n    # make it a 1d array\n    target = np.ravel(target)\n    print target.size\n    \n    #Split into training and test sets\n    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, random_state=0)\n    \n    max_depth = [9]\n    # min_child_weight = [i for i in range(1,8)]\n    subsample = [0.4,0.5,0.6,0.7]\n    n_estimators = [100]\n    learning_rate  = [0.05]\n    # gamma = [i/10.0 for i in range(0,5)]\n    # parameters = {'max_depth':max_depth,'n_estimators':n_estimators,'learning_rate':learning_rate,'min_child_weight':min_child_weight,'subsample':subsample}\n    parameters = {'max_depth':max_depth,'n_estimators':n_estimators,'learning_rate':learning_rate,'subsample':subsample}\n    # parameters = {'n_estimators':n_estimators,'learning_rate':learning_rate}\n    \n    seed = 342\n    np.random.seed(seed)\n    \n    cv = StratifiedKFold(target_train, n_folds = 5, shuffle = True, random_state = seed)\n    XGBC=XGBClassifier(seed = seed)\n    grid_obj = GridSearchCV(XGBC, parameters,cv=cv, scoring='roc_auc')\n    \n    # TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n    grid_fit = grid_obj.fit(features_train,target_train)\n    \n    # Get the estimator\n    best_clf = grid_fit.best_estimator_\n    \n    print grid_fit.best_params_ \n    \n    # Make predictions using the unoptimized and model\n    model = best_clf.fit(features_train, target_train)\n    \n    # predict the outcome for the test features\n    predicted = model.predict(features_test)\n    print predicted\n    \n    # Check to see if it identifies anyone as churning\n    predicted.size\n    np.sum(predicted)\n    \n    probs = model.predict_proba(features_test)\n    print probs\n    \n    print metrics.accuracy_score(target_test, predicted)\n    # The metric we use in leaderboard\n    print metrics.roc_auc_score(target_test, probs[:, 1])\n    \n    print metrics.confusion_matrix(target_test, predicted)\n    print metrics.classification_report(target_test, predicted)\n    return model\n\n " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Train the model using data till 2017-07-31\n# Create the training dataset\ndf = create_training_dataset('2017-07-31')\n\nimport numpy as np\ndf.drop(['snapshot_day'],axis = 1,inplace = True)\nprint \"before\"\nprint df.shape\nfrom scipy import stats\ndf = df[(np.abs(stats.zscore(df)) < 5).all(axis=1)]\nprint \"final\"\nprint df.shape\n\nactual_status = get_customer_status('2017-08-31', '2017-07-31')\n# Join with the actual status\ndfx = df.join(actual_status.to_frame(), how='left')\n# dfx.head()\n# Save it in s3\nwrite_to_s3(dfx, 'ml-hack-subs-data', 'training-dataset-2017-07-31.csv')\n# To retrieve the file via Eider save it in /tmp\ndfx.to_csv('/tmp/training-dataset-2017-07-31.csv')\n\n# Train the model and get the artifact\n# model = create_model(dfx)\n# model = createRandomForestmodel(dfx)\n# model = createGradientBoostingModel(dfx)\n\n# # Create the feature data for the validation set\n# df_contest = create_training_dataset('2017-08-31')\n\n# # Filter the feature df to include only the features used in the model\n# difffeatures = ['diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n# difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n# deviceFeatures = ['active_android', 'active_eink', 'active_kcp', 'active_ios', 'active_kcr', 'active_tablet', 'active_phone']\n# contest_input = df_contest[['subs_age', 'cancelledRatio', 'snapVals', 'books_30', 'books_90', 'ku_loans_t7', 'ku_loans_mtd', 'pbook_units_mtd'] + difffeatures + deviceFeatures + difffeaturespaidunits]\n# # Predict the probabilities\n# contest_probs = model.predict_proba(contest_input)\n# # Create the submission file in the required format\n# submission = pd.DataFrame(contest_probs[:,1])\n# submission.columns = ['ku_status']\n# submission.set_index(contest_input.index, inplace=True)\n# submission.index.name = 'ID'\n# submission.head()\n# # Save the file to Eider TMP, to retrieve it from the FILES page\n# submission.to_csv('/tmp/contest-submission.csv')\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# dfx.hist(layout = (40,2), figsize = (10,70))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# # submission.head(5)\n# model = createRandomForestmodel(dfx)\n# model = createGradientBoostingModel(dfx)\nmodel = createXGBoostModel(dfx)\n\n# Create the feature data for the validation set\ndf_contest = create_training_dataset('2017-08-31')\n\n# Filter the feature df to include only the features used in the model\ndifffeatures = ['diffm7tom6kuloans','diffm6tom5kuloans','diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\ndifffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\ndifffeaturespbookunits = ['diffm5tom4pbookunits','diffm4tom3pbookunits','diffm3tom2pbookunits','diffm2tom1pbookunits','diffm1tom0pbookunits']\ndifffeaturesfbookunits = ['diffm5tom4fbookunits','diffm4tom3fbookunits','diffm3tom2fbookunits','diffm2tom1fbookunits','diffm1tom0fbookunits']\n\ndeviceFeatures = ['active_android', 'active_eink', 'active_kcp', 'active_ios', 'active_kcr', 'active_tablet', 'active_phone']\ncontest_input = df_contest[['lastReadAge', 'bookreads', 'books_180', 'lostbratio', 'time_diff', 'max_read', 'subs_age', 'cancelledRatio', 'snapVals', 'books_30', 'books_90', 'ku_loans_t7', 'ku_loans_mtd', 'pbook_units_mtd'] + difffeaturespaidunits + difffeatures + deviceFeatures + difffeaturespbookunits + difffeaturesfbookunits]\n# Predict the probabilities\ncontest_probs = model.predict_proba(contest_input)\n# Create the submission file in the required format\nsubmission = pd.DataFrame(contest_probs[:,1])\nsubmission.columns = ['ku_status']\nsubmission.set_index(contest_input.index, inplace=True)\nsubmission.index.name = 'ID'\nsubmission.head()\n# Save the file to Eider TMP, to retrieve it from the FILES page\nsubmission.to_csv('/tmp/contest-submission.csv')" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# df_contest = create_training_dataset('2017-08-31')\n\n# # Filter the feature df to include only the features used in the model\n# difffeatures = ['diffm7tom6kuloans','diffm6tom5kuloans','diffm5tom4kuloans','diffm4tom3kuloans','diffm3tom2kuloans','diffm2tom1kuloans','diffm1tom0kuloans']\n# difffeaturespaidunits = ['diffm5tom4paidunits','diffm4tom3paidunits','diffm3tom2paidunits','diffm2tom1paidunits','diffm1tom0paidunits']\n# difffeaturespbookunits = ['diffm5tom4pbookunits','diffm4tom3pbookunits','diffm3tom2pbookunits','diffm2tom1pbookunits','diffm1tom0pbookunits']\n# difffeaturesfbookunits = ['diffm5tom4fbookunits','diffm4tom3fbookunits','diffm3tom2fbookunits','diffm2tom1fbookunits','diffm1tom0fbookunits']\n\n# deviceFeatures = ['active_android', 'active_eink', 'active_kcp', 'active_ios', 'active_kcr', 'active_tablet', 'active_phone']\n\n# contest_input = df_contest[['bookreads', 'books_180', 'lostbratio', 'time_diff', 'max_read', 'subs_age', 'cancelledRatio', 'snapVals', 'books_30', 'books_90', 'ku_loans_t7', 'ku_loans_mtd', 'pbook_units_mtd'] + difffeaturespaidunits + difffeatures + deviceFeatures + difffeaturespbookunits + difffeaturesfbookunits]\n# # Predict the probabilities\n# contest_probs = model.predict_proba(contest_input)\n# # Create the submission file in the required format\n# submission = pd.DataFrame(contest_probs[:,1])\n# submission.columns = ['ku_status']\n# submission.set_index(contest_input.index, inplace=True)\n# submission.index.name = 'ID'\n# submission.head()\n# Save the file to Eider TMP, to retrieve it from the FILES page\nsubmission.to_csv('/tmp/contest-submission.csv')" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "submission.to_csv('/tmp/contest-submission.csv')\nfrom xgboost import plot_importance\n\nplot_importance(model)\n# pyplot.figure(figsize=(50,10))\nmng = pyplot.get_current_fig_manager()\nmng.frame.Maximize(True)\npyplot.show()\n\nfrom xgboost import plot_tree\n# plot single tree\nplot_tree(model, num_trees=0)\npyplot.figure()\npyplot.show()" ]
  } ]
}